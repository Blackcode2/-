{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110a03f2-81e1-4b16-9741-4a1ae7416d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 100708.0234\n",
      "   10 | 40935.8203\n",
      "   20 | 16639.9238\n",
      "   30 |  6764.2539\n",
      "   40 |  2750.0454\n",
      "   50 |  1118.3710\n",
      "   60 |   455.1349\n",
      "   70 |   185.5464\n",
      "   80 |    75.9656\n",
      "   90 |    31.4235\n",
      "  100 |    13.3182\n",
      "  110 |     5.9588\n",
      "  120 |     2.9672\n",
      "  130 |     1.7510\n",
      "  140 |     1.2565\n",
      "  150 |     1.0554\n",
      "  160 |     0.9735\n",
      "  170 |     0.9401\n",
      "  180 |     0.9264\n",
      "  190 |     0.9207\n",
      "  200 |     0.9182\n",
      "  210 |     0.9171\n",
      "  220 |     0.9165\n",
      "  230 |     0.9161\n",
      "  240 |     0.9158\n",
      "  250 |     0.9156\n",
      "  260 |     0.9153\n",
      "  270 |     0.9151\n",
      "  280 |     0.9148\n",
      "  290 |     0.9146\n",
      "  300 |     0.9144\n",
      "  310 |     0.9142\n",
      "  320 |     0.9139\n",
      "  330 |     0.9137\n",
      "  340 |     0.9135\n",
      "  350 |     0.9132\n",
      "  360 |     0.9130\n",
      "  370 |     0.9128\n",
      "  380 |     0.9126\n",
      "  390 |     0.9123\n",
      "  400 |     0.9121\n",
      "  410 |     0.9119\n",
      "  420 |     0.9117\n",
      "  430 |     0.9114\n",
      "  440 |     0.9112\n",
      "  450 |     0.9110\n",
      "  460 |     0.9108\n",
      "  470 |     0.9105\n",
      "  480 |     0.9103\n",
      "  490 |     0.9101\n",
      "  500 |     0.9099\n",
      "  510 |     0.9096\n",
      "  520 |     0.9094\n",
      "  530 |     0.9092\n",
      "  540 |     0.9090\n",
      "  550 |     0.9087\n",
      "  560 |     0.9085\n",
      "  570 |     0.9083\n",
      "  580 |     0.9081\n",
      "  590 |     0.9078\n",
      "  600 |     0.9076\n",
      "  610 |     0.9074\n",
      "  620 |     0.9072\n",
      "  630 |     0.9069\n",
      "  640 |     0.9067\n",
      "  650 |     0.9065\n",
      "  660 |     0.9063\n",
      "  670 |     0.9060\n",
      "  680 |     0.9058\n",
      "  690 |     0.9056\n",
      "  700 |     0.9054\n",
      "  710 |     0.9051\n",
      "  720 |     0.9049\n",
      "  730 |     0.9047\n",
      "  740 |     0.9045\n",
      "  750 |     0.9042\n",
      "  760 |     0.9040\n",
      "  770 |     0.9038\n",
      "  780 |     0.9036\n",
      "  790 |     0.9034\n",
      "  800 |     0.9031\n",
      "  810 |     0.9029\n",
      "  820 |     0.9027\n",
      "  830 |     0.9025\n",
      "  840 |     0.9023\n",
      "  850 |     0.9020\n",
      "  860 |     0.9018\n",
      "  870 |     0.9016\n",
      "  880 |     0.9013\n",
      "  890 |     0.9011\n",
      "  900 |     0.9009\n",
      "  910 |     0.9007\n",
      "  920 |     0.9005\n",
      "  930 |     0.9002\n",
      "  940 |     0.9000\n",
      "  950 |     0.8998\n",
      "  960 |     0.8996\n",
      "  970 |     0.8993\n",
      "  980 |     0.8991\n",
      "  990 |     0.8989\n",
      " 1000 |     0.8987\n",
      " 1010 |     0.8985\n",
      " 1020 |     0.8982\n",
      " 1030 |     0.8980\n",
      " 1040 |     0.8978\n",
      " 1050 |     0.8976\n",
      " 1060 |     0.8973\n",
      " 1070 |     0.8971\n",
      " 1080 |     0.8969\n",
      " 1090 |     0.8967\n",
      " 1100 |     0.8965\n",
      " 1110 |     0.8963\n",
      " 1120 |     0.8960\n",
      " 1130 |     0.8958\n",
      " 1140 |     0.8956\n",
      " 1150 |     0.8954\n",
      " 1160 |     0.8951\n",
      " 1170 |     0.8949\n",
      " 1180 |     0.8947\n",
      " 1190 |     0.8945\n",
      " 1200 |     0.8943\n",
      " 1210 |     0.8940\n",
      " 1220 |     0.8938\n",
      " 1230 |     0.8936\n",
      " 1240 |     0.8934\n",
      " 1250 |     0.8932\n",
      " 1260 |     0.8930\n",
      " 1270 |     0.8927\n",
      " 1280 |     0.8925\n",
      " 1290 |     0.8923\n",
      " 1300 |     0.8921\n",
      " 1310 |     0.8919\n",
      " 1320 |     0.8916\n",
      " 1330 |     0.8914\n",
      " 1340 |     0.8912\n",
      " 1350 |     0.8910\n",
      " 1360 |     0.8908\n",
      " 1370 |     0.8906\n",
      " 1380 |     0.8903\n",
      " 1390 |     0.8901\n",
      " 1400 |     0.8899\n",
      " 1410 |     0.8897\n",
      " 1420 |     0.8895\n",
      " 1430 |     0.8893\n",
      " 1440 |     0.8890\n",
      " 1450 |     0.8888\n",
      " 1460 |     0.8886\n",
      " 1470 |     0.8884\n",
      " 1480 |     0.8882\n",
      " 1490 |     0.8880\n",
      " 1500 |     0.8877\n",
      " 1510 |     0.8875\n",
      " 1520 |     0.8873\n",
      " 1530 |     0.8871\n",
      " 1540 |     0.8869\n",
      " 1550 |     0.8867\n",
      " 1560 |     0.8864\n",
      " 1570 |     0.8862\n",
      " 1580 |     0.8860\n",
      " 1590 |     0.8858\n",
      " 1600 |     0.8856\n",
      " 1610 |     0.8854\n",
      " 1620 |     0.8852\n",
      " 1630 |     0.8849\n",
      " 1640 |     0.8847\n",
      " 1650 |     0.8845\n",
      " 1660 |     0.8843\n",
      " 1670 |     0.8841\n",
      " 1680 |     0.8839\n",
      " 1690 |     0.8837\n",
      " 1700 |     0.8835\n",
      " 1710 |     0.8833\n",
      " 1720 |     0.8831\n",
      " 1730 |     0.8828\n",
      " 1740 |     0.8826\n",
      " 1750 |     0.8824\n",
      " 1760 |     0.8822\n",
      " 1770 |     0.8820\n",
      " 1780 |     0.8818\n",
      " 1790 |     0.8816\n",
      " 1800 |     0.8814\n",
      " 1810 |     0.8812\n",
      " 1820 |     0.8809\n",
      " 1830 |     0.8807\n",
      " 1840 |     0.8805\n",
      " 1850 |     0.8803\n",
      " 1860 |     0.8801\n",
      " 1870 |     0.8799\n",
      " 1880 |     0.8797\n",
      " 1890 |     0.8795\n",
      " 1900 |     0.8793\n",
      " 1910 |     0.8791\n",
      " 1920 |     0.8788\n",
      " 1930 |     0.8786\n",
      " 1940 |     0.8784\n",
      " 1950 |     0.8782\n",
      " 1960 |     0.8780\n",
      " 1970 |     0.8778\n",
      " 1980 |     0.8776\n",
      " 1990 |     0.8774\n",
      " 2000 |     0.8772\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data = np.array([\n",
    "    [ 73., 80., 75., 152. ],\n",
    "    [ 93., 88., 93., 185. ],\n",
    "    [ 89., 91., 90., 180. ],\n",
    "    [ 96., 98., 100., 196 ],\n",
    "    [ 73., 66., 70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "#slice data\n",
    "x = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "w = tf.Variable(tf.random.normal([3,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learing_rate = 0.000001\n",
    "\n",
    "#hypothesis, prediction function\n",
    "def predict(x):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "epoch = 2000\n",
    "for i in range(epoch+1):\n",
    "    # record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean((tf.square(predict(x) - y))) # 궁금한점: 손실함수의 값들은 행렬로 나와야 하는것 아닌가? 왜 하나의 값으로 출력?\n",
    "        \n",
    "    # calculates the gradients of the loss\n",
    "    w_grad, b_grad = tape.gradient(loss, [w, b])\n",
    "    \n",
    "    #updates parameters (w and b)\n",
    "    w.assign_sub(learing_rate * w_grad)\n",
    "    b.assign_sub(learing_rate * b_grad)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, loss.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
